{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import scipy.io\n",
    "#import numpy as np\n",
    "#import statistics, math\n",
    "import matplotlib.pyplot as plt\n",
    "#from libs import util_matlab as umatlab\n",
    "#from libs import datasets, dataset_utils, utils\n",
    "import tensorflow as tf\n",
    "\n",
    "# We'll tell matplotlib to inline any drawn figures like so:\n",
    "%matplotlib notebook\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of sequences in a mini batch\n",
    "batch_size = 100\n",
    "\n",
    "# Number of characters in a sequence\n",
    "sequence_length = 12\n",
    "\n",
    "# Number of cells in our LSTM layer\n",
    "n_cells = 256\n",
    "\n",
    "# Number of LSTM layers\n",
    "n_layers = 1\n",
    "\n",
    "n_inputs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, n_inputs], name='X')\n",
    "\n",
    "# We'll have a placeholder for our true outputs\n",
    "Y = tf.placeholder(tf.float32, [None, sequence_length], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(12), Dimension(8)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if n_layers > 1:\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "        [cells] * n_layers, state_is_tuple=True)\n",
    "    initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)\n",
    "else:\n",
    "    cells = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_cells, state_is_tuple=True)\n",
    "    initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 12, 8]\n",
      "[12, None, 8]\n",
      "[None, 8]\n",
      "[<tf.Tensor 'split_2:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:1' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:2' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:3' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:4' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:5' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:6' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:7' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:8' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:9' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:10' shape=(?, 8) dtype=float32>, <tf.Tensor 'split_2:11' shape=(?, 8) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "#tf.nn.rnn connects the Layer (variable cells) to the inputs (X) through tf.nn.rnn's second input parameter:\n",
    "#tf.nn.rnn(cells, inputs=X, initial_state=initial_state)\n",
    "#BUT parameter 'inputs=X' should be a list!\n",
    "#So, convert X to a list:\n",
    "print(X.get_shape().as_list())\n",
    "# Permuting batch_size and n_steps. TO: (sequence_length, batch_size, n_inputs)\n",
    "Xs = tf.transpose(X, [1, 0, 2])\n",
    "print(Xs.get_shape().as_list())\n",
    "# Reshaping to (sequence_length*batch_size, n_inputs)\n",
    "Xs = tf.reshape(Xs, [-1, n_inputs])\n",
    "print(Xs.get_shape().as_list())\n",
    "# Split to get a list of 'sequence_length' tensors of shape (batch_size, n_input)\n",
    "Xs = tf.split(0, sequence_length, Xs)\n",
    "print(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'split:0' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:1' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:2' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:3' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:4' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:5' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:6' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:7' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:8' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:9' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:10' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'split:11' shape=(?, 8) dtype=float32>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will return us a list of outputs of every element in our sequence.\n",
    "# Each output is `batch_size` x `n_cells` of output.\n",
    "# It will also return the state as a tuple of the n_cells's memory and\n",
    "# their output to connect to the time we use the recurrent layer.\n",
    "outputs, state = tf.nn.rnn(cells, Xs, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll now stack all our outputs for every cell\n",
    "outputs_flat = tf.reshape(tf.concat(1, outputs), [-1, n_cells])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
